{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 06:05:07 WARN Utils: Your hostname, 005482.local resolves to a loopback address: 127.0.0.1; using 192.168.1.38 instead (on interface en0)\n",
      "24/03/06 06:05:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/06 06:05:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/06 06:05:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_type = \"yellow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       1| 2020-01-01 00:28:15|  2020-01-01 00:33:03|            1.0|          1.2|       1.0|                 N|         238|         239|           1|        6.0|  3.0|    0.5|      1.47|         0.0|                  0.3|       11.27|                 2.5|       NULL|\n",
      "|       1| 2020-01-01 00:35:39|  2020-01-01 00:43:04|            1.0|          1.2|       1.0|                 N|         239|         238|           1|        7.0|  3.0|    0.5|       1.5|         0.0|                  0.3|        12.3|                 2.5|       NULL|\n",
      "|       1| 2020-01-01 00:47:41|  2020-01-01 00:53:52|            1.0|          0.6|       1.0|                 N|         238|         238|           1|        6.0|  3.0|    0.5|       1.0|         0.0|                  0.3|        10.8|                 2.5|       NULL|\n",
      "|       1| 2020-01-01 00:55:23|  2020-01-01 01:00:14|            1.0|          0.8|       1.0|                 N|         238|         151|           1|        5.5|  0.5|    0.5|      1.36|         0.0|                  0.3|        8.16|                 0.0|       NULL|\n",
      "|       2| 2020-01-01 00:01:58|  2020-01-01 00:04:16|            1.0|          0.0|       1.0|                 N|         193|         193|           2|        3.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         4.8|                 0.0|       NULL|\n",
      "|       2| 2020-01-01 00:09:44|  2020-01-01 00:10:37|            1.0|         0.03|       1.0|                 N|           7|         193|           2|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         3.8|                 0.0|       NULL|\n",
      "|       2| 2020-01-01 00:39:25|  2020-01-01 00:39:29|            1.0|          0.0|       1.0|                 N|         193|         193|           1|        2.5|  0.5|    0.5|      0.01|         0.0|                  0.3|        3.81|                 0.0|       NULL|\n",
      "|       2| 2019-12-18 15:27:49|  2019-12-18 15:28:59|            1.0|          0.0|       5.0|                 N|         193|         193|           1|       0.01|  0.0|    0.0|       0.0|         0.0|                  0.3|        2.81|                 2.5|       NULL|\n",
      "|       2| 2019-12-18 15:30:35|  2019-12-18 15:31:35|            4.0|          0.0|       1.0|                 N|         193|         193|           1|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         6.3|                 2.5|       NULL|\n",
      "|       1| 2020-01-01 00:29:01|  2020-01-01 00:40:28|            2.0|          0.7|       1.0|                 N|         246|          48|           1|        8.0|  3.0|    0.5|      2.35|         0.0|                  0.3|       14.15|                 2.5|       NULL|\n",
      "|       1| 2020-01-01 00:55:11|  2020-01-01 01:12:03|            2.0|          2.4|       1.0|                 N|         246|          79|           1|       12.0|  3.0|    0.5|      1.75|         0.0|                  0.3|       17.55|                 2.5|       NULL|\n",
      "|       1| 2020-01-01 00:37:15|  2020-01-01 00:51:41|            1.0|          0.8|       1.0|                 N|         163|         161|           2|        9.5|  3.0|    0.5|       0.0|         0.0|                  0.3|        13.3|                 2.5|       NULL|\n",
      "|       1| 2020-01-01 00:56:27|  2020-01-01 01:21:44|            1.0|          3.3|       1.0|                 N|         161|         144|           1|       17.0|  3.0|    0.5|      4.15|         0.0|                  0.3|       24.95|                 2.5|       NULL|\n",
      "|       2| 2020-01-01 00:21:54|  2020-01-01 00:27:31|            1.0|         1.07|       1.0|                 N|          43|         239|           1|        6.0|  0.5|    0.5|      1.96|         0.0|                  0.3|       11.76|                 2.5|       NULL|\n",
      "|       2| 2020-01-01 00:38:01|  2020-01-01 01:15:21|            1.0|         7.76|       1.0|                 N|         143|          25|           1|       28.5|  0.5|    0.5|      4.84|         0.0|                  0.3|       37.14|                 2.5|       NULL|\n",
      "|       1| 2020-01-01 00:15:35|  2020-01-01 00:27:06|            3.0|          1.6|       1.0|                 N|         211|         234|           2|        9.0|  3.0|    0.5|       0.0|         0.0|                  0.3|        12.8|                 2.5|       NULL|\n",
      "|       1| 2020-01-01 00:41:20|  2020-01-01 00:44:22|            1.0|          0.5|       1.0|                 Y|         234|          90|           1|        4.0|  3.0|    0.5|       1.0|         0.0|                  0.3|         8.8|                 2.5|       NULL|\n",
      "|       1| 2020-01-01 00:56:38|  2020-01-01 01:13:34|            1.0|          1.7|       1.0|                 N|         246|         142|           2|       11.5|  3.0|    0.5|       0.0|         0.0|                  0.3|        15.3|                 2.5|       NULL|\n",
      "|       2| 2020-01-01 00:08:21|  2020-01-01 00:25:29|            1.0|         8.45|       1.0|                 N|         138|         216|           2|       24.5|  0.5|    0.5|       0.0|         0.0|                  0.3|        25.8|                 0.0|       NULL|\n",
      "|       1| 2020-01-01 00:25:39|  2020-01-01 00:27:05|            1.0|          0.0|       1.0|                 N|         170|         162|           4|        3.0|  3.0|    0.5|       0.0|         0.0|                  0.3|         6.8|                 2.5|       NULL|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(f\"data/raw/{service_type}/*/*\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'RatecodeID',\n",
       " 'store_and_fwd_flag',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'payment_type',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'congestion_surcharge',\n",
       " 'airport_fee']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n",
    "if \"lpep_pickup_datetime\" in df.columns:\n",
    "    df = df.withColumnRenamed(\n",
    "        \"lpep_pickup_datetime\", \"pickup_datetime\"\n",
    "    ).withColumnRenamed(\"lpep_dropoff_datetime\", \"dropoff_datetime\")\n",
    "elif \"tpep_pickup_datetime\" in df.columns:\n",
    "    df = df.withColumnRenamed(\n",
    "        \"tpep_pickup_datetime\", \"pickup_datetime\"\n",
    "    ).withColumnRenamed(\"tpep_dropoff_datetime\", \"dropoff_datetime\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue_hourly = spark.sql(\n",
    "    \"\"\"\n",
    "SELECT \n",
    "    date_trunc('hour', pickup_datetime) AS hour,\n",
    "    PULocationID as zone,\n",
    "    \n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(1) as record_count\n",
    "FROM \n",
    "    table\n",
    "GROUP BY \n",
    "    1,2\n",
    "ORDER BY amount DESC\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:==============================================>          (18 + 4) / 22]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+------------------+------------+\n",
      "|               hour|zone|            amount|record_count|\n",
      "+-------------------+----+------------------+------------+\n",
      "|2020-03-10 09:00:00| 193|1000026.4500000001|           3|\n",
      "|2020-10-07 10:00:00|  41|         998602.27|          21|\n",
      "|2021-09-05 18:00:00| 141|  819787.340000001|         107|\n",
      "|2020-03-04 17:00:00| 166| 673021.1200000009|         117|\n",
      "|2020-05-04 20:00:00| 142|429733.24999999994|          12|\n",
      "|2022-01-07 11:00:00| 107|402162.56999999995|          78|\n",
      "|2021-03-18 12:00:00| 161|401589.70999999973|         228|\n",
      "|2020-12-26 13:00:00| 170|399255.83999999997|          57|\n",
      "|2021-04-10 13:00:00| 234|398795.42999999964|         184|\n",
      "|2022-06-11 09:00:00| 163|397751.55999999953|         100|\n",
      "|2022-09-24 17:00:00| 233|         189123.76|          92|\n",
      "|2020-08-14 17:00:00| 142|188859.90999999983|          79|\n",
      "|2020-11-17 06:00:00|  41|151590.91999999995|           8|\n",
      "|2022-12-29 23:00:00| 132| 41493.56000000006|         567|\n",
      "|2020-01-05 21:00:00| 132| 40236.33999999998|         725|\n",
      "|2022-12-30 16:00:00| 132| 39936.44000000005|         460|\n",
      "|2022-12-29 16:00:00| 132| 39134.33000000004|         465|\n",
      "|2020-01-26 20:00:00| 132|38895.609999999935|         708|\n",
      "|2022-12-27 22:00:00| 132| 38398.02000000001|         520|\n",
      "|2020-01-20 18:00:00| 132| 38346.72999999995|         693|\n",
      "+-------------------+----+------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "revenue_hourly.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "revenue_hourly.coalesce(1).write.parquet(\n",
    "    f\"data/report/revenue/{service_type}\", mode=\"overwrite\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_hourly = spark.read.parquet(\"data/report/revenue/green/*\")\n",
    "yellow_hourly = spark.read.parquet(\"data/report/revenue/yellow/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_green_type = green_hourly.withColumn(\"service_type\", F.lit(\"green\"))\n",
    "df_yellow_type = yellow_hourly.withColumn(\"service_type\", F.lit(\"yellow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_hourly.createOrReplaceTempView(\"green_hourly\")\n",
    "yellow_hourly.createOrReplaceTempView(\"yellow_hourly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue_hourly = spark.sql(\n",
    "    \"\"\"\n",
    "SELECT \n",
    "    g.hour,\n",
    "    g.zone,\n",
    "    \n",
    "    ROUND(g.amount+y.amount, 2) AS total_amount,\n",
    "    g.record_count+y.record_count AS total_count,\n",
    "\n",
    "    ROUND(g.amount, 2) green_amount,\n",
    "    ROUND(y.amount, 2) yellow_amount,\n",
    "    g.record_count green_count,\n",
    "    y.record_count yellow_count\n",
    "FROM \n",
    "    green_hourly g\n",
    "JOIN\n",
    "    yellow_hourly y\n",
    "ON \n",
    "    g.hour = y.hour \n",
    "    AND g.zone = y.zone\n",
    "ORDER BY total_amount DESC\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+------------+-----------+------------+-------------+-----------+------------+\n",
      "|               hour|zone|total_amount|total_count|green_amount|yellow_amount|green_count|yellow_count|\n",
      "+-------------------+----+------------+-----------+------------+-------------+-----------+------------+\n",
      "|2020-03-10 09:00:00| 193|  1000097.68|          8|       71.23|   1000026.45|          5|           3|\n",
      "|2020-10-07 10:00:00|  41|   998718.75|         31|      116.48|    998602.27|         10|          21|\n",
      "|2020-03-04 17:00:00| 166|    673987.3|        177|      966.18|    673021.12|         60|         117|\n",
      "|2020-11-17 06:00:00|  41|   151682.54|         13|       91.62|    151590.92|          5|           8|\n",
      "|2020-01-26 20:00:00| 132|    38904.91|        709|         9.3|     38895.61|          1|         708|\n",
      "|2020-01-27 16:00:00| 132|    35945.89|        589|         9.8|     35936.09|          1|         588|\n",
      "|2020-03-01 20:00:00| 132|    34817.04|        623|       59.92|     34757.12|          1|         622|\n",
      "|2020-02-17 20:00:00| 132|    34683.25|        633|         8.3|     34674.95|          1|         632|\n",
      "|2022-12-28 15:00:00| 132|    34669.55|        443|         6.6|     34662.95|          1|         442|\n",
      "|2022-12-30 17:00:00| 132|    34287.69|        426|        46.2|     34241.49|          2|         424|\n",
      "|2020-01-20 14:00:00| 132|     33550.5|        596|       42.36|     33508.14|          1|         595|\n",
      "|2020-01-20 15:00:00| 132|    33529.51|        592|         6.3|     33523.21|          1|         591|\n",
      "|2022-12-28 18:00:00| 132|    32680.18|        403|       99.45|     32580.73|          1|         402|\n",
      "|2022-12-22 16:00:00| 132|    32406.44|        385|       124.5|     32281.94|          2|         383|\n",
      "|2020-01-11 18:00:00| 132|    32235.06|        585|       62.88|     32172.18|          1|         584|\n",
      "|2022-09-05 17:00:00| 132|     32159.2|        514|      120.72|     32038.48|          2|         512|\n",
      "|2022-11-03 16:00:00| 132|    31563.99|        456|       66.49|      31497.5|          1|         455|\n",
      "|2022-09-05 19:00:00| 132|    31174.24|        504|        28.3|     31145.94|          1|         503|\n",
      "|2020-02-10 16:00:00| 132|    30696.19|        491|       14.16|     30682.03|          1|         490|\n",
      "|2020-01-12 18:00:00| 132|    30624.54|        568|       62.92|     30561.62|          1|         567|\n",
      "+-------------------+----+------------+-----------+------------+-------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "revenue_hourly.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_hourly_tmp = green_hourly.withColumnRenamed(\n",
    "    \"amount\", \"green_amount\"\n",
    ").withColumnRenamed(\"record_count\", \"green_record_count\")\n",
    "\n",
    "yellow_hourly_tmp = yellow_hourly.withColumnRenamed(\n",
    "    \"amount\", \"yellow_amount\"\n",
    ").withColumnRenamed(\"record_count\", \"yellow_record_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join = green_hourly_tmp.join(yellow_hourly_tmp, on=[\"hour\", \"zone\"], how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 06:05:28 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "24/03/06 06:05:28 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "24/03/06 06:05:28 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "24/03/06 06:05:28 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "24/03/06 06:05:28 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "24/03/06 06:05:28 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 58.46% for 13 writers\n",
      "24/03/06 06:05:28 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 54.29% for 14 writers\n",
      "24/03/06 06:05:28 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 50.67% for 15 writers\n",
      "24/03/06 06:05:28 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 47.50% for 16 writers\n",
      "24/03/06 06:05:28 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 50.67% for 15 writers\n",
      "24/03/06 06:05:28 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 54.29% for 14 writers\n",
      "24/03/06 06:05:28 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 58.46% for 13 writers\n",
      "24/03/06 06:05:28 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "24/03/06 06:05:28 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "24/03/06 06:05:28 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "24/03/06 06:05:28 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "24/03/06 06:05:28 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "24/03/06 06:05:28 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "24/03/06 06:05:28 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_join.write.parquet(\"data/report/revenue/total\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+------------+------------------+-------------+-------------------+\n",
      "|               hour|zone|green_amount|green_record_count|yellow_amount|yellow_record_count|\n",
      "+-------------------+----+------------+------------------+-------------+-------------------+\n",
      "|2001-01-01 01:00:00| 230|        NULL|              NULL|         24.8|                  1|\n",
      "|2002-10-21 01:00:00| 230|        NULL|              NULL|         20.8|                  1|\n",
      "|2002-10-21 18:00:00| 138|        NULL|              NULL|        260.3|                  1|\n",
      "|2002-10-22 08:00:00| 226|        NULL|              NULL|         24.8|                  1|\n",
      "|2002-10-22 19:00:00| 158|        NULL|              NULL|         13.0|                  1|\n",
      "|2002-10-22 23:00:00| 186|        NULL|              NULL|         13.8|                  1|\n",
      "|2002-10-23 00:00:00| 264|        NULL|              NULL|          6.8|                  1|\n",
      "|2002-10-23 04:00:00|  79|        NULL|              NULL|         25.8|                  1|\n",
      "|2002-10-23 09:00:00| 141|        NULL|              NULL|         12.3|                  1|\n",
      "|2002-10-23 10:00:00| 161|        NULL|              NULL|         10.3|                  1|\n",
      "|2002-10-23 13:00:00|  24|        NULL|              NULL|         24.3|                  1|\n",
      "|2002-10-23 13:00:00| 152|        NULL|              NULL|         17.8|                  1|\n",
      "|2002-10-23 14:00:00| 138|        NULL|              NULL|         49.6|                  1|\n",
      "|2002-10-23 18:00:00| 262|        NULL|              NULL|          9.8|                  1|\n",
      "|2002-10-24 13:00:00| 138|        NULL|              NULL|         67.6|                  1|\n",
      "|2002-10-25 07:00:00| 162|        NULL|              NULL|         15.3|                  1|\n",
      "|2002-10-25 09:00:00|   1|        NULL|              NULL|         85.3|                  1|\n",
      "|2002-10-25 14:00:00| 163|        NULL|              NULL|        84.75|                  1|\n",
      "|2002-10-25 21:00:00|  40|        NULL|              NULL|          0.0|                  2|\n",
      "|2002-10-26 22:00:00| 255|        NULL|              NULL|         20.3|                  1|\n",
      "+-------------------+----+------------+------------------+-------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read.csv(\n",
    "    \"data/zones/taxi_zone_lookup.csv\", header=True, inferSchema=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones.write.parquet(\"data/zones/taxi_zone_lookup.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read.parquet(\"data/zones/taxi_zone_lookup.parquet\").withColumnRenamed(\n",
    "    \"LocationID\", \"zone\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+--------------------+------------+\n",
      "|zone|      Borough|                Zone|service_zone|\n",
      "+----+-------------+--------------------+------------+\n",
      "|   1|          EWR|      Newark Airport|         EWR|\n",
      "|   2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|   3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|   4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|   5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|   6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|   7|       Queens|             Astoria|   Boro Zone|\n",
      "|   8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|   9|       Queens|          Auburndale|   Boro Zone|\n",
      "|  10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|  11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|  12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|  13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|  14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|  15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|  16|       Queens|             Bayside|   Boro Zone|\n",
      "|  17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|  18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|  19|       Queens|           Bellerose|   Boro Zone|\n",
      "|  20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zones.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join_zone = df_join.join(df_zones, on=\"zone\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+------------+------------------+-------------+-------------------+---------+--------------------+------------+\n",
      "|zone|               hour|green_amount|green_record_count|yellow_amount|yellow_record_count|  Borough|                Zone|service_zone|\n",
      "+----+-------------------+------------+------------------+-------------+-------------------+---------+--------------------+------------+\n",
      "| 230|2001-01-01 01:00:00|        NULL|              NULL|         24.8|                  1|Manhattan|Times Sq/Theatre ...| Yellow Zone|\n",
      "| 230|2002-10-21 01:00:00|        NULL|              NULL|         20.8|                  1|Manhattan|Times Sq/Theatre ...| Yellow Zone|\n",
      "| 138|2002-10-21 18:00:00|        NULL|              NULL|        260.3|                  1|   Queens|   LaGuardia Airport|    Airports|\n",
      "| 226|2002-10-22 08:00:00|        NULL|              NULL|         24.8|                  1|   Queens|           Sunnyside|   Boro Zone|\n",
      "| 158|2002-10-22 19:00:00|        NULL|              NULL|         13.0|                  1|Manhattan|Meatpacking/West ...| Yellow Zone|\n",
      "| 186|2002-10-22 23:00:00|        NULL|              NULL|         13.8|                  1|Manhattan|Penn Station/Madi...| Yellow Zone|\n",
      "| 264|2002-10-23 00:00:00|        NULL|              NULL|          6.8|                  1|  Unknown|                 N/A|         N/A|\n",
      "|  79|2002-10-23 04:00:00|        NULL|              NULL|         25.8|                  1|Manhattan|        East Village| Yellow Zone|\n",
      "| 141|2002-10-23 09:00:00|        NULL|              NULL|         12.3|                  1|Manhattan|     Lenox Hill West| Yellow Zone|\n",
      "| 161|2002-10-23 10:00:00|        NULL|              NULL|         10.3|                  1|Manhattan|      Midtown Center| Yellow Zone|\n",
      "|  24|2002-10-23 13:00:00|        NULL|              NULL|         24.3|                  1|Manhattan|        Bloomingdale| Yellow Zone|\n",
      "| 152|2002-10-23 13:00:00|        NULL|              NULL|         17.8|                  1|Manhattan|      Manhattanville|   Boro Zone|\n",
      "| 138|2002-10-23 14:00:00|        NULL|              NULL|         49.6|                  1|   Queens|   LaGuardia Airport|    Airports|\n",
      "| 262|2002-10-23 18:00:00|        NULL|              NULL|          9.8|                  1|Manhattan|      Yorkville East| Yellow Zone|\n",
      "| 138|2002-10-24 13:00:00|        NULL|              NULL|         67.6|                  1|   Queens|   LaGuardia Airport|    Airports|\n",
      "| 162|2002-10-25 07:00:00|        NULL|              NULL|         15.3|                  1|Manhattan|        Midtown East| Yellow Zone|\n",
      "|   1|2002-10-25 09:00:00|        NULL|              NULL|         85.3|                  1|      EWR|      Newark Airport|         EWR|\n",
      "| 163|2002-10-25 14:00:00|        NULL|              NULL|        84.75|                  1|Manhattan|       Midtown North| Yellow Zone|\n",
      "|  40|2002-10-25 21:00:00|        NULL|              NULL|          0.0|                  2| Brooklyn|     Carroll Gardens|   Boro Zone|\n",
      "| 255|2002-10-26 22:00:00|        NULL|              NULL|         20.3|                  1| Brooklyn|Williamsburg (Nor...|   Boro Zone|\n",
      "+----+-------------------+------------+------------------+-------------+-------------------+---------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join_zone.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = df.select(\"pickup_datetime\", \"PULocationID\", \"total_amount\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(pickup_datetime=datetime.datetime(2020, 1, 1, 0, 28, 15), PULocationID=238, total_amount=11.27),\n",
       " Row(pickup_datetime=datetime.datetime(2020, 1, 1, 0, 35, 39), PULocationID=239, total_amount=12.3),\n",
       " Row(pickup_datetime=datetime.datetime(2020, 1, 1, 0, 47, 41), PULocationID=238, total_amount=10.8),\n",
       " Row(pickup_datetime=datetime.datetime(2020, 1, 1, 0, 55, 23), PULocationID=238, total_amount=8.16),\n",
       " Row(pickup_datetime=datetime.datetime(2020, 1, 1, 0, 1, 58), PULocationID=193, total_amount=4.8),\n",
       " Row(pickup_datetime=datetime.datetime(2020, 1, 1, 0, 9, 44), PULocationID=7, total_amount=3.8),\n",
       " Row(pickup_datetime=datetime.datetime(2020, 1, 1, 0, 39, 25), PULocationID=193, total_amount=3.81),\n",
       " Row(pickup_datetime=datetime.datetime(2019, 12, 18, 15, 27, 49), PULocationID=193, total_amount=2.81),\n",
       " Row(pickup_datetime=datetime.datetime(2019, 12, 18, 15, 30, 35), PULocationID=193, total_amount=6.3),\n",
       " Row(pickup_datetime=datetime.datetime(2020, 1, 1, 0, 29, 1), PULocationID=246, total_amount=14.15)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(year=2020, month=1, day=1)\n",
    "\n",
    "\n",
    "def filter_outliers(row):\n",
    "    return row.pickup_datetime >= start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(pickup_datetime=datetime.datetime(2020, 1, 1, 0, 28, 15), PULocationID=238, total_amount=11.27)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = rdd.take(10)\n",
    "row = rows[0]\n",
    "\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_grouping(row):\n",
    "    hour = row.pickup_datetime.replace(minute=0, second=0, microsecond=0)\n",
    "    zone = row.PULocationID\n",
    "    key = (hour, zone)\n",
    "\n",
    "    amount = row.total_amount\n",
    "    count = 1\n",
    "    value = (amount, count)\n",
    "\n",
    "    return (key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((datetime.datetime(2020, 1, 1, 0, 0), 238), (11.27, 1))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_for_grouping(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_revenue(left_value, right_value):\n",
    "    left_amount, left_count = left_value\n",
    "    right_amount, right_count = right_value\n",
    "\n",
    "    output_amount = left_amount + right_amount\n",
    "    output_count = left_count + right_count\n",
    "\n",
    "    return (output_amount, output_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "RevenueRow = namedtuple(\"RevenueRow\", [\"hour\", \"zone\", \"revenue\", \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwrap(row):\n",
    "    return RevenueRow(\n",
    "        hour=row[0][0], zone=row[0][1], revenue=row[1][0], count=row[1][1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_schema = types.StructType(\n",
    "    [\n",
    "        types.StructField(\"hour\", types.TimestampType(), True),\n",
    "        types.StructField(\"zone\", types.IntegerType(), True),\n",
    "        types.StructField(\"revenue\", types.DoubleType(), True),\n",
    "        types.StructField(\"count\", types.IntegerType(), True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = (\n",
    "    rdd.filter(filter_outliers)\n",
    "    .map(prepare_for_grouping)\n",
    "    .reduceByKey(calculate_revenue)\n",
    "    .map(unwrap)\n",
    "    .toDF(result_schema)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hour', TimestampType(), True), StructField('zone', IntegerType(), True), StructField('revenue', DoubleType(), True), StructField('count', IntegerType(), True)])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:=====================================================>  (21 + 1) / 22]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+------------------+-----+\n",
      "|               hour|zone|           revenue|count|\n",
      "+-------------------+----+------------------+-----+\n",
      "|2020-01-01 00:00:00| 236| 5254.590000000019|  339|\n",
      "|2020-01-01 00:00:00| 142|  9252.30000000001|  488|\n",
      "|2020-01-01 00:00:00|  90|  5010.45000000001|  266|\n",
      "|2020-01-01 00:00:00| 260|             74.14|    8|\n",
      "|2020-01-01 01:00:00|  90| 4326.070000000006|  231|\n",
      "|2020-01-01 00:00:00|  60|57.620000000000005|    2|\n",
      "|2020-01-01 01:00:00| 234| 6217.040000000017|  347|\n",
      "|2020-01-01 01:00:00| 134|              9.75|    1|\n",
      "|2020-01-01 02:00:00|  75|1314.5599999999984|   85|\n",
      "|2020-01-01 02:00:00| 249| 6182.610000000019|  299|\n",
      "|2020-01-01 01:00:00| 228|             74.07|    2|\n",
      "|2020-01-01 02:00:00|  41|1480.5899999999983|   91|\n",
      "|2020-01-01 02:00:00| 145| 596.0499999999998|   32|\n",
      "|2020-01-01 02:00:00| 125|1753.1899999999994|   74|\n",
      "|2020-01-01 03:00:00| 137|2045.3099999999977|  116|\n",
      "|2020-01-01 02:00:00| 179| 300.6100000000002|   23|\n",
      "|2020-01-01 02:00:00| 159|            137.69|    5|\n",
      "|2020-01-01 02:00:00| 165|              38.3|    1|\n",
      "|2020-01-01 03:00:00|  47|29.900000000000002|    3|\n",
      "|2020-01-01 03:00:00| 181| 761.0100000000001|   37|\n",
      "+-------------------+----+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 06:06:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "24/03/06 06:06:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "24/03/06 06:06:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "24/03/06 06:06:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "24/03/06 06:06:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "24/03/06 06:06:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 58.46% for 13 writers\n",
      "24/03/06 06:06:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 54.29% for 14 writers\n",
      "24/03/06 06:06:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 50.67% for 15 writers\n",
      "24/03/06 06:06:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 47.50% for 16 writers\n",
      "24/03/06 06:07:14 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 50.67% for 15 writers\n",
      "24/03/06 06:07:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 54.29% for 14 writers\n",
      "24/03/06 06:07:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 58.46% for 13 writers\n",
      "24/03/06 06:07:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "24/03/06 06:07:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "24/03/06 06:07:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "24/03/06 06:07:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "24/03/06 06:07:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result.write.parquet(\"tmp/value\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"VendorID\",\n",
    "    \"pickup_datetime\",\n",
    "    \"PULocationID\",\n",
    "    \"DOLocationID\",\n",
    "    \"trip_distance\",\n",
    "]\n",
    "service_type = \"green\"\n",
    "df = spark.read.parquet(f\"data/raw/{service_type}/*/*\")\n",
    "df.columns\n",
    "if \"lpep_pickup_datetime\" in df.columns:\n",
    "    df = df.withColumnRenamed(\n",
    "        \"lpep_pickup_datetime\", \"pickup_datetime\"\n",
    "    ).withColumnRenamed(\"lpep_dropoff_datetime\", \"dropoff_datetime\")\n",
    "elif \"tpep_pickup_datetime\" in df.columns:\n",
    "    df = df.withColumnRenamed(\n",
    "        \"tpep_pickup_datetime\", \"pickup_datetime\"\n",
    "    ).withColumnRenamed(\"tpep_dropoff_datetime\", \"dropoff_datetime\")\n",
    "\n",
    "duration_rdd = df.select(columns).rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(VendorID=2, pickup_datetime=datetime.datetime(2019, 12, 18, 15, 52, 30), PULocationID=264, DOLocationID=264, trip_distance=0.0),\n",
       " Row(VendorID=2, pickup_datetime=datetime.datetime(2020, 1, 1, 0, 45, 58), PULocationID=66, DOLocationID=65, trip_distance=1.28),\n",
       " Row(VendorID=2, pickup_datetime=datetime.datetime(2020, 1, 1, 0, 41, 38), PULocationID=181, DOLocationID=228, trip_distance=2.47),\n",
       " Row(VendorID=1, pickup_datetime=datetime.datetime(2020, 1, 1, 0, 52, 46), PULocationID=129, DOLocationID=263, trip_distance=6.3),\n",
       " Row(VendorID=1, pickup_datetime=datetime.datetime(2020, 1, 1, 0, 19, 57), PULocationID=210, DOLocationID=150, trip_distance=2.3),\n",
       " Row(VendorID=1, pickup_datetime=datetime.datetime(2020, 1, 1, 0, 52, 33), PULocationID=35, DOLocationID=39, trip_distance=3.0),\n",
       " Row(VendorID=2, pickup_datetime=datetime.datetime(2020, 1, 1, 0, 10, 18), PULocationID=25, DOLocationID=61, trip_distance=2.77),\n",
       " Row(VendorID=2, pickup_datetime=datetime.datetime(2020, 1, 1, 1, 3, 14), PULocationID=225, DOLocationID=89, trip_distance=4.98),\n",
       " Row(VendorID=2, pickup_datetime=datetime.datetime(2020, 1, 1, 0, 4, 11), PULocationID=129, DOLocationID=129, trip_distance=0.71),\n",
       " Row(VendorID=2, pickup_datetime=datetime.datetime(2020, 1, 1, 0, 25, 52), PULocationID=129, DOLocationID=83, trip_distance=0.8)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infinite_seq():\n",
    "    i = 0\n",
    "    while True:\n",
    "        yield i\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = infinite_seq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "for i in seq:\n",
    "    print(i)\n",
    "\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def model_predict(df):\n",
    "    # y_pred = model.predict(df)\n",
    "    y_pred = df.trip_distance * 5\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def apply_model_in_batch(partition):\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    predictions = model_predict(df)\n",
    "    df[\"predicted_duration\"] = predictions\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 06:10:03 ERROR Executor: Exception in task 0.0 in stage 50.0 (TID 307)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py\", line 939, in _finalize_columns_and_data\n",
      "    columns = _validate_or_indexify_columns(contents, columns)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py\", line 986, in _validate_or_indexify_columns\n",
      "    raise AssertionError(\n",
      "AssertionError: 5 columns passed, passed data had 3 columns\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pyspark/rdd.py\", line 2849, in takeUpToNumLeft\n",
      "    yield next(iterator)\n",
      "          ^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/wp/fzyspr3j2lq6vw4th77_dm4h0000gq/T/ipykernel_41277/2195418199.py\", line 11, in apply_model_in_batch\n",
      "  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/frame.py\", line 840, in __init__\n",
      "    arrays, columns, index = nested_data_to_arrays(\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py\", line 520, in nested_data_to_arrays\n",
      "    arrays, columns = to_arrays(data, columns, dtype=dtype)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py\", line 845, in to_arrays\n",
      "    content, columns = _finalize_columns_and_data(arr, columns, dtype)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py\", line 942, in _finalize_columns_and_data\n",
      "    raise ValueError(err) from err\n",
      "ValueError: 5 columns passed, passed data had 3 columns\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/03/06 06:10:03 WARN TaskSetManager: Lost task 0.0 in stage 50.0 (TID 307) (192.168.1.38 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py\", line 939, in _finalize_columns_and_data\n",
      "    columns = _validate_or_indexify_columns(contents, columns)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py\", line 986, in _validate_or_indexify_columns\n",
      "    raise AssertionError(\n",
      "AssertionError: 5 columns passed, passed data had 3 columns\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pyspark/rdd.py\", line 2849, in takeUpToNumLeft\n",
      "    yield next(iterator)\n",
      "          ^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/wp/fzyspr3j2lq6vw4th77_dm4h0000gq/T/ipykernel_41277/2195418199.py\", line 11, in apply_model_in_batch\n",
      "  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/frame.py\", line 840, in __init__\n",
      "    arrays, columns, index = nested_data_to_arrays(\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py\", line 520, in nested_data_to_arrays\n",
      "    arrays, columns = to_arrays(data, columns, dtype=dtype)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py\", line 845, in to_arrays\n",
      "    content, columns = _finalize_columns_and_data(arr, columns, dtype)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py\", line 942, in _finalize_columns_and_data\n",
      "    raise ValueError(err) from err\n",
      "ValueError: 5 columns passed, passed data had 3 columns\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/03/06 06:10:03 ERROR TaskSetManager: Task 0 in stage 50.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 50.0 failed 1 times, most recent failure: Lost task 0.0 in stage 50.0 (TID 307) (192.168.1.38 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py\", line 939, in _finalize_columns_and_data\n    columns = _validate_or_indexify_columns(contents, columns)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py\", line 986, in _validate_or_indexify_columns\n    raise AssertionError(\nAssertionError: 5 columns passed, passed data had 3 columns\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pyspark/rdd.py\", line 2849, in takeUpToNumLeft\n    yield next(iterator)\n          ^^^^^^^^^^^^^^\n  File \"/var/folders/wp/fzyspr3j2lq6vw4th77_dm4h0000gq/T/ipykernel_41277/2195418199.py\", line 11, in apply_model_in_batch\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/frame.py\", line 840, in __init__\n    arrays, columns, index = nested_data_to_arrays(\n                             ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py\", line 520, in nested_data_to_arrays\n    arrays, columns = to_arrays(data, columns, dtype=dtype)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py\", line 845, in to_arrays\n    content, columns = _finalize_columns_and_data(arr, columns, dtype)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py\", line 942, in _finalize_columns_and_data\n    raise ValueError(err) from err\nValueError: 5 columns passed, passed data had 3 columns\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py\", line 939, in _finalize_columns_and_data\n    columns = _validate_or_indexify_columns(contents, columns)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py\", line 986, in _validate_or_indexify_columns\n    raise AssertionError(\nAssertionError: 5 columns passed, passed data had 3 columns\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pyspark/rdd.py\", line 2849, in takeUpToNumLeft\n    yield next(iterator)\n          ^^^^^^^^^^^^^^\n  File \"/var/folders/wp/fzyspr3j2lq6vw4th77_dm4h0000gq/T/ipykernel_41277/2195418199.py\", line 11, in apply_model_in_batch\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/frame.py\", line 840, in __init__\n    arrays, columns, index = nested_data_to_arrays(\n                             ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py\", line 520, in nested_data_to_arrays\n    arrays, columns = to_arrays(data, columns, dtype=dtype)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py\", line 845, in to_arrays\n    content, columns = _finalize_columns_and_data(arr, columns, dtype)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py\", line 942, in _finalize_columns_and_data\n    raise ValueError(err) from err\nValueError: 5 columns passed, passed data had 3 columns\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mduration_rdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapply_model_in_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoDF\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pyspark/sql/session.py:122\u001b[0m, in \u001b[0;36m_monkey_patch_RDD.<locals>.toDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;129m@no_type_check\u001b[39m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtoDF\u001b[39m(\u001b[38;5;28mself\u001b[39m, schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sampleRatio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     89\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    Converts current :class:`RDD` into a :class:`DataFrame`\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m    +---+\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampleRatio\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pyspark/sql/session.py:1443\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m   1442\u001b[0m     )\n\u001b[0;32m-> 1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1445\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pyspark/sql/session.py:1483\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1480\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m   1482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, RDD):\n\u001b[0;32m-> 1483\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromRDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n",
      "File \u001b[0;32m~/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pyspark/sql/session.py:1056\u001b[0m, in \u001b[0;36mSparkSession._createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;124;03mCreate an RDD for DataFrame from an existing RDD, returns the RDD and schema.\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m-> 1056\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[1;32m   1058\u001b[0m     tupled_rdd \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmap(converter)\n",
      "File \u001b[0;32m~/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pyspark/sql/session.py:996\u001b[0m, in \u001b[0;36mSparkSession._inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_inferSchema\u001b[39m(\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    977\u001b[0m     rdd: RDD[Any],\n\u001b[1;32m    978\u001b[0m     samplingRatio: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    979\u001b[0m     names: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    980\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m StructType:\n\u001b[1;32m    981\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;124;03m    Infer schema from an RDD of Row, dict, or tuple.\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;124;03m    :class:`pyspark.sql.types.StructType`\u001b[39;00m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 996\u001b[0m     first \u001b[38;5;241m=\u001b[39m \u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first, Sized) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(first) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    998\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m    999\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_INFER_EMPTY_SCHEMA\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1000\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m   1001\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pyspark/rdd.py:2888\u001b[0m, in \u001b[0;36mRDD.first\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2862\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[T]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m   2863\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2864\u001b[0m \u001b[38;5;124;03m    Return the first element in this RDD.\u001b[39;00m\n\u001b[1;32m   2865\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2886\u001b[0m \u001b[38;5;124;03m    ValueError: RDD is empty\u001b[39;00m\n\u001b[1;32m   2887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2888\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2889\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[1;32m   2890\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pyspark/rdd.py:2855\u001b[0m, in \u001b[0;36mRDD.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   2852\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2854\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[0;32m-> 2855\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2857\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m   2858\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[0;32m~/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pyspark/context.py:2510\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   2508\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[1;32m   2509\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2510\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 50.0 failed 1 times, most recent failure: Lost task 0.0 in stage 50.0 (TID 307) (192.168.1.38 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py\", line 939, in _finalize_columns_and_data\n    columns = _validate_or_indexify_columns(contents, columns)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py\", line 986, in _validate_or_indexify_columns\n    raise AssertionError(\nAssertionError: 5 columns passed, passed data had 3 columns\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pyspark/rdd.py\", line 2849, in takeUpToNumLeft\n    yield next(iterator)\n          ^^^^^^^^^^^^^^\n  File \"/var/folders/wp/fzyspr3j2lq6vw4th77_dm4h0000gq/T/ipykernel_41277/2195418199.py\", line 11, in apply_model_in_batch\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/frame.py\", line 840, in __init__\n    arrays, columns, index = nested_data_to_arrays(\n                             ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py\", line 520, in nested_data_to_arrays\n    arrays, columns = to_arrays(data, columns, dtype=dtype)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py\", line 845, in to_arrays\n    content, columns = _finalize_columns_and_data(arr, columns, dtype)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py\", line 942, in _finalize_columns_and_data\n    raise ValueError(err) from err\nValueError: 5 columns passed, passed data had 3 columns\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py\", line 939, in _finalize_columns_and_data\n    columns = _validate_or_indexify_columns(contents, columns)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py\", line 986, in _validate_or_indexify_columns\n    raise AssertionError(\nAssertionError: 5 columns passed, passed data had 3 columns\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pyspark/rdd.py\", line 2849, in takeUpToNumLeft\n    yield next(iterator)\n          ^^^^^^^^^^^^^^\n  File \"/var/folders/wp/fzyspr3j2lq6vw4th77_dm4h0000gq/T/ipykernel_41277/2195418199.py\", line 11, in apply_model_in_batch\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/frame.py\", line 840, in __init__\n    arrays, columns, index = nested_data_to_arrays(\n                             ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py\", line 520, in nested_data_to_arrays\n    arrays, columns = to_arrays(data, columns, dtype=dtype)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py\", line 845, in to_arrays\n    content, columns = _finalize_columns_and_data(arr, columns, dtype)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jonathondonager/Documents/GitHub/data-engineering-learn/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py\", line 942, in _finalize_columns_and_data\n    raise ValueError(err) from err\nValueError: 5 columns passed, passed data had 3 columns\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "duration_rdd.mapPartitions(apply_model_in_batch).toDF().take(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
